## Hadoop 集群搭建

### 配置 Hadoop 环境变量

使用 Filezilla 将 `hadoop-2.7.6.tar.gz` 上传到 /usr/local 目录下

解压 hadoop-2.7.6.tar.gz 包

````
tar -zxvf hadoop-2.7.6.tar.gz
````

删除 /usr/local 下 的 hadoop-2.7.6.tar.gz 包

````
rm -rf hadoop-2.7.6.tar.gz
````

对 hadoop-2.7.7-src 进行重命名

````
mv hadoop-2.7.6 hadoop
````

配置 Hadoop 环境变量

````
vi /etc/profile

export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
````

### 修改 Hadoop 配置

#### 1. 修改  core-site.xml 

进入 /usr/local/hadoop/etc/hadoop 目录下，编辑 core-site.xml 文件

````xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
</configuration>
````

改成

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <property>
    	<name>fs.default.name</name>
        <value>hdfs://spark1:9000</value>
    </property>
</configuration>
```

这就是 hdfs 集群对外提供的服务

#### 2. 修改 hdfs-site.xml

进入 /usr/local/hadoop/etc/hadoop 目录下，编辑 hdfs-site.xml 文件

````xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>

</configuration>
````

改成

````xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>dfs.name.dir</name>
        <value>/usr/local/data/namenode</value>
    </property>
    <property>
        <name>dfs.data.dir</name>
        <value>/usr/local/data/datanode</value>
    </property>
    <property>
        <name>dfs.tmp.dir</name>
        <value>/usr/local/data/tmp</value>
    </property>
    <property>
        <name>dfs.replication.dir</name>
        <value>3</value>
    </property>
</configuration>
````

设置 hdfs 的目录

因为设置存放目录为 /usr/local/data，所以在 /usr/local 目录下，创建一个 data 目录

````
cd /usr/local/
mkdir data
````

#### 3. 修改 mapred-site.xml

````xml
<property>
    <name>dfs.replication.dir</name>
    <value>yarn</value>
</property>
````

#### 4.修改 yarn-site.xml

````xml
<property>
    <name>yarn.resourcemanager.hostname</name>
    <value>spark1</value>
</property>
<property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
</property>
````

#### 5. 修改 slaves 文件

````
spark1
spark2
spark3
````

### 在另外 2 台机器上搭建 hadoop

1. 使用如上配置在另外 2 台机器上搭建 hadoop，可以使用 scp 命令将 spark1 上面的 hadoop 安装包和 /etc/profile 配置文件拷贝过去。
2. 要记得对 /etc/profile 文件进行 source，以让它生效。
3. 还要在 spark2 和 spark3 的 /usr/local/ 下创建 data 目录。

````xml
scp -r hadoop root@spark2:/usr/local/
scp -r /etc/profile root@spark2:/etc/
source /etc/profile
````
### 启动 hdfs 集群

1. 格式化 namenode:在 spark1 上执行以下命令 hdfs namenode -format

2. 启动 hdfs 集群：start-dfs.sh

3. 验证启动是否成功：jps 50070 端口

    访问 http://spark1:50070/ 浏览器，可以看到相关配置

    ````python
    # jps 验证结果
    
    spark1:
       5340 DataNode
       5219 NameNode
       5957 Jps
       5490 SecondaryNameNode
    spark2:
       11262 DataNode
       11324 Jps
    
    spark3:
    	11340 Jps
    	11278 DataNode
    ````
4. 执行 start-yarn.sh

   ````
   spark1:
       3728 DataNode
       3916 SecondaryNameNode
       3625 NameNode
       4190 NodeManager
       4091 ResourceManager
       4534 Jps
   spark3:
       3492 DataNode
       3731 Jps
       3603 NodeManager
   spark2:
       3730 Jps
       3496 DataNode
       3603 NodeManager
   ````
> **注意：**
> 配置环境变量必须在 bashrc 配置，在 /etc/profile 上配置以后，start-dfs.sh 时，提示 JAVA_HOME not set or not found
>
> 在 spark1、spark2、spark3 上重新配置环境变量即可。

```java
vi /etc/bashrc
source /etc/bashrc

export JAVA_HOME=/usr/java/jdk1.7.0_80
export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export PATH=$PATH:$JAVA_HOME/bin

export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

#### 解决 start-dfs.sh 启动以后，spark节点没有 datanode，只有namenode。

到 /usr/local 目录下的 /tmp/dfs 可以看到两个目录：namenode 和 datanode，分别找到该目录下的VERSION 文件，查看其 clusterID，修改 datanode 中的 clusterID 使其与 namenode 中的 clusterID 保持相同。如果相同，则 datanode 可正常启动。

````python
# namenode
namespaceID=1483272486
clusterID=CID-2659f7b4-8161-4f09-8da0-0c26f712286f
cTime=0
storageType=NAME_NODE
blockpoolID=BP-1191073750-10.15.0.43-1546675639124
layoutVersion=-63

#datanode
storageID=DS-df09079d-409a-48af-8c8e-210ff84f1ed3
clusterID=CID-e09b4579-71ab-4210-9fc9-6cd825a304ca
cTime=0
datanodeUuid=a388c477-9257-4099-8022-d869831516b6
storageType=DATA_NODE
layoutVersion=-56
````

#### 解决 ssh:connect to host : connected refused

[ssh:connect to host : connected refused](https://blog.csdn.net/jszhangyili/article/details/8881807)

1. sshd 未安装

2. sshd 未启动

3. 防火墙（优先检查防火墙）
4. 需重新启动 ssh 服务

**解决方法**

1. 确定安装 sshd

````
sudo apt-get install openssh-server
````

2. 启动 sshd

````
sudo net start sshd
````

3. 检查防火墙设置，关闭防火墙

````python
关闭FirewallD防火墙
#systemctl stop firewalld

检查FirewallD防火墙状态
#systemctl status firewalld
#firewall-cmd --state
````

检验方法

````
ssh spark2
````

